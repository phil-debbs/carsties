dotnet tool install dotnet-ef -g
dotnet ef migrations add "InitialCreate" -o Data/Migrations
docker compose up -d
dotnet ef database update
dotnet ef database drop
dotnet watch  //this runs seeding for the database if included
dotnet new gitignore

dotnet new webapi -o src/SearchService  //creates a new webapi project
dotnet sln add  src/SearchService  //adds SearchService project to sln

dotnet add reference ../../src/Contracts

docker pull postgres

docker image ls

docker run \
  --name pgsql-dev \
  –rm \
  -e POSTGRES_PASSWORD=test1234 \
  -p 5432:5432 postgres

docker run -d -e POSTGRES_PASSWORD=postgrespw -p 5431:5431 -v ${PWD}/postgres-docker:/var/lib/postgresql/data postgres


docker run --name postgresTest -d -e POSTGRES_PASSWORD=postgrespw -p 5432:5432  postgres

–name: the name of the PostgreSQL container.
–rm: this removes the container when it’s stopped.
-e: the only mandatory environment variable is the database password that needs to be provided before creating the container.
-p: the port mapping needs to be provided so that the host port on the machine will map to the PostgreSQL container port inside the container.

https://earthly.dev/blog/postgres-docker/#:~:text=5s%20retries%3A%205-,Limitations%20of%20Running%20PostgreSQL%20Database%20with%20Docker,great%20for%20running%20stateless%20applications.

Basically, you need to map the data directory of the PostgreSQL from inside the container to a directory on your local machine. This can be done using the following command:
$ docker run \
  --name pgsql-dev \
  -e POSTGRES_PASSWORD=test1234 \
  -d \
  -v ${PWD}/postgres-docker:/var/lib/postgresql/data \
  -p 5432:5432 postgres 

-Detached mode: denoted by d, the detached mode will allow the container to run in the background.
-Volume: the local directory on the host is mapped to the data directory for PostgreSQL inside the container.


services:
  postgres:
    image: postgres:14-alpine
    ports:
      - 5432:5432
    volumes:
      - ~/apps/postgres:/var/lib/postgresql/data
    environment:
      - POSTGRES_PASSWORD=S3cret
      - POSTGRES_USER=citizix_user
      - POSTGRES_DB=citizix_db

Elasticsearch
https://www.knowi.com/blog/what-is-elastic-search/
docker run -d --name elasticsearch --net somenetwork -p 9200:9200 -p 9300:9300 -e "discovery.type=single-node" elasticsearch:tag

docker run -d --name elasticsearch --net somenetwork -p 9200:9200 -p 9300:9300 -e "discovery.type=single-node" elasticsearch:tag


Postgres Docker Best Practices

1.Backup your data periodically. You can do this by running the pg_dump command from the database container:

docker exec -it <container_name> \
  pg_dump -U<user_name> --column-inserts --data-only <db_name> > \
  backup_data.sql

2.Use alpine images if possible. They’re usually smaller in size. For instance, postgres:14.2 is 131mb in size whereas postgres:14.2-alpine is only 78mb with the same functionality. Additionally, alpine images are secure because all the userspace binaries are compiled to protect against common vulnerabilities.

3.Use a persistent volume to store data. As mentioned above, without a persistent volume, you’ll lose data if the container restarts.

4.If there is no database when PostgreSQL starts in a container, a default database will be created and it will not accept incoming connections during that time. This may cause issues with automation tools which may try to access the database as soon as the container starts. To mitigate this, you need to ensure that the database is accepting connections before trying to connect to it. If you’re using Docker Compose, you can use the healthcheck feature:
healthcheck:
       test: ["CMD-SHELL", "pg_isready -U postgres"]
       interval: 5s
       timeout: 5s
       retries: 5